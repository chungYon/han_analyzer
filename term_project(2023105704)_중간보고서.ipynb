{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f66f78a",
   "metadata": {},
   "source": [
    "# 주제 : 온라인 한글 사용빈도 수 분석 및 허프만 코딩 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75474ee",
   "metadata": {},
   "source": [
    "## 1. 주제 선택 이유\n",
    "데이터 분석과 웹 크롤링으로 할 수 있는 일이 무엇이 있을까 궁금하던 찰나, 온라인에서 한글 부호화에 대한 논문을 읽게 되었습니다. \n",
    "기존 유니코드 기반의 한글 처리가 지나치게 많은 부호를 사용한다고 주장하면서, 최적화된 한글 부호화를 위해 한국어 사용빈도를 데이터를 분석한 내용을 담고 있습니다. 저는 이 논문이 기반으로 하는 국립국어원 데이터셋이 과연 모든 온라인 사이트에 적용이 될 수 있는지 궁금했습니다. 먼저 '기존 데이터'를 앞으로 국립국어원이 작성한 데이터라고 지칭하겠습니다. <br> 저의 생각을 말하자면 국립국어원의 잡지, 신문을 기반으로 도출한 결과와는 꽤나 많은 차이가 날 것이라 예상합니다. 따라서 대표적인 사이트 몇 개를 추린 다음, 거기에서 각각 나온 한국어 사용빈도를 추출하고 이를 기존 논문의 데이터셋과 비교하여 얼마나 차이가 있는지 확인하는 과정을 거칠 예정입니다. 또한 허프만 코딩을 구현해 실제 사이트에 적용한 결과를 기존 데이터를 적용한 결과와 비교/분석할 것입니다.<br>\n",
    "## 2. 가설 정의\n",
    "제가 온라인에서 추출한 데이터와 기존 데이터 사이에는 굉장히 많은 차이가 날 것으로 생각합니다. 그렇게 생각한 이유로는 온라인 매체에서는 단어선택이 오프라인 매체에 비해 상대적으로 자유롭기 때문입니다. 오프라인의 양식이 그대로 온라인 양식으로 옮겨간 사이트(ex. 중앙일보, 네이버기사, 등등)는 기존 데이터셋의 결과를 따를 수 있겠지만, 오프라인 매체에서 유래하지 않은 독자적인 온라인 사이트(ex. 커뮤니티, sns 등)에서 주로 사용하는 언어는 꽤나 다릅니다. 예를 들어 웃거나 울때는 'ㅋ'과 'ㅠ'을 여러번 타이핑하거나, 공식적인 매체에서 볼 수 없는 은어나 비속어의 사용이 그렇습니다. 온라인 사이트에서의 독자적인 단어인지 검출하는 기준은 국립국어원의 표준어 기준을 기반으로 하겠습니다. 저는 기존 데이터에서 빈도순 기준으로 후 순위에 위치했던 자음이나 모음이 인터넷 기반 데이터셋에서는 일부 전복되는 모습이 보여질 것 입니다. 하지만 문장을 구성하는 필수적인 요소에 들어가는 자음이나 모음은 필수요소에 많이 들어갈수록 기존 데이터와 거의 차이가 없거나 기존 데이터에 온라인 사이트의 독자적인 양식에 들어가는 자음/모음 만큼 가산되어 나타날 것입니다.\n",
    "따라서 웹사이트에서 추출한 빈도수를 기반으로 작성한 허프만 인코딩 방식이 기존데이터 기반보다 더 효율적으로 가동할 것 같습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9790c2d5",
   "metadata": {},
   "source": [
    "## 3. 인터넷을 통한 데이터 획득\n",
    "국립국어원 데이터셋은 이미 txt파일로 인터넷에 배포되어 있습니다. 문제는 자음/모음의 빈도 수는 고려하지 않았기 때문에 파이썬 파일 입출력 기능으로 일일히 단어하나 하나를 분해하여 단어 하나당 자음/모음의 개수에 단어의 개수를 곱하여 총 낱자의 빈도수를 계산하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b525f074",
   "metadata": {},
   "source": [
    "웹사이트로 직접 들어갈때 추출할 표본의 개수는 500개로 제한하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c66a65",
   "metadata": {},
   "source": [
    "### a. 웹 크롤링 기능 만들기\n",
    "\n",
    "웹크롤링을 구현하기 위해 이미 널리 알려져있는 Selenium과 BeautifulSoup를 기반으로 작성하도록 하겠습니다.<br>\n",
    "본격적으로 코딩하기에 앞서 만들고자 하는 클래스의 기능을 정리해보았습니다.<br><br>\n",
    ". 클래스 이름 : parse_contents<br>\n",
    ". 클래스 기능 : \n",
    "1. 클래스가 생성됨과 동시에 입력받은 url로 접속하여 특정 게시판의 글들을 긁어옵니다. \n",
    "2. 글을 띄어쓰기 기준으로 분할하여 단어의 개수를 정리합니다. \n",
    "3. 정리된 단어의 개수를 토대로 자음/모음의 개수를 계산하고 이를 변수에 저장합니다. \n",
    "4. 메소드가 호출될 때 저장된 변수를 리턴합니다.\n",
    "\n",
    ". 클래스 변수:\n",
    "1. word_count : 파싱한 단어의 개수를 저장한 딕셔너리\n",
    "2. vowel_count : 파싱한 모음의 개수를 저장한 딕셔너리\n",
    "3. consonants_count : 파싱한 자음의 개수를 저장한 딕셔너리\n",
    "4. visit_posting_title_list : 방문할 게시판의 제목을 모아 놓은 리스트\n",
    "5. visit_posiing_webelement_list : 방문할 게시판의 웹 엘리먼트 클래스를 모아놓은 리스트\n",
    "6. posting_board_raw_text : 방문한 게시판마다 긁어온 본문 문자열들의 리스트를 저장하는 딕셔너리\n",
    "7. base_url : 크롤링할 웹사이트의 기본 주소\n",
    "8. posting_board_url_list : 게시판 주소를 모아놓은 리스트\n",
    "\n",
    ". 메소드 : \n",
    "1. get_word_count : 단어 개수를 저장한 딕셔너리를 반환\n",
    "2. get_vowel_count : 모음 개수를 저장한 딕셔너리를 반환\n",
    "3. _getVisitPostingBoardAdress : 메인페이지에서 파싱하여 각 게시판의 주소리스트를 반환\n",
    "4. parsing_text_list : 게시판의 본문 파싱\n",
    "5. counting_words : 본문의 단어 개수세기\n",
    "6. counting_vowels_consonants : 본문의 자음, 모음 개수세기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51bf81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from time import time, sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class EveryTime_parse_contents:\n",
    "    url = 'https://everytime.kr/login'\n",
    "\n",
    "    headers = {\n",
    "    \"User-Agent\":\n",
    "    \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36\"\n",
    "    }\n",
    "    \n",
    "    payload = {'param1': '1', 'param2': '2'}\n",
    "\n",
    "    response = requests.get(url, params=payload, headers=headers)\n",
    "    \n",
    "    def __init__(self, visit_board_list_t):\n",
    "        self.visit_board_list = visit_board_list_t\n",
    "\n",
    "        self.driver = webdriver.Chrome()\n",
    "        self.driver.implicitly_wait(1)\n",
    "        self.driver.get(EveryTime_parse_contents.url)\n",
    "\n",
    "        input('계속하려면 아무키나 누르시오')\n",
    "        self.driver.implicitly_wait(1)\n",
    "        self.posting_board_url_list = self._getVisitPostingBoardAdress()\n",
    "        \n",
    "        print(self.posting_board_url_list)\n",
    "        \n",
    "        self._getPostingBoardTexts()\n",
    "    \n",
    "    #원하는 게시판의 주소를 저장하는 리스트를 반환하는 메소드가 필요함\n",
    "    def _getVisitPostingBoardAdress(self):\n",
    "        main_div = self.driver.find_element(By.CLASS_NAME, 'main')\n",
    "        posting_div_list = main_div.find_elements(By.CLASS_NAME, 'board')\n",
    "        \n",
    "        posting_board_url_list_ = []\n",
    "        for idiv in posting_div_list:\n",
    "            posting_board_aTag = idiv.find_element(By.TAG_NAME, 'h3').find_element(By.TAG_NAME, 'a')\n",
    "            posting_board_title_text = posting_board_aTag.text\n",
    "            \n",
    "            if(posting_board_title_text in self.visit_board_list):\n",
    "                posting_board_url_list_.append(posting_board_aTag.get_attribute('href'))\n",
    "                \n",
    "        return posting_board_url_list_\n",
    "    \n",
    "    def _getPostingBoardTexts(self):\n",
    "        \n",
    "        board_article_url = []\n",
    "        \n",
    "        for iurl in self.posting_board_url_list:\n",
    "            self.driver.get(iurl)\n",
    "            sleep(10)\n",
    "            articles_div = self.driver.find_element(By.CLASS_NAME, 'wrap.articles')\n",
    "            article_element_list = articles_div.find_elements(By.TAG_NAME, 'article')\n",
    "            article_url_list = []\n",
    "            \n",
    "            #한 페이지당 20개의 게시글\n",
    "            article_url_list = []\n",
    "            for iarticle in article_element_list:\n",
    "                article_url = iarticle.find_element(By.CLASS_NAME, 'article').get_attribute('href')\n",
    "                article_url_list.append(article_url)\n",
    "            board_article_url.append(article_url_list)\n",
    "        \n",
    "        for i in board_article_url:\n",
    "            for iarticle_url in i:\n",
    "                self.driver.get(iarticle_url)\n",
    "                self.driver.implicitly_wait(1)\n",
    "                large_elements = self.driver.find_elements(By.CLASS_NAME, 'large')\n",
    "                article_content_list.append(large_elements[0].text + ' ' + large_elements[1].text)\n",
    "                self.driver.back()\n",
    "    \n",
    "p = EveryTime_parse_contents(['국제캠 자유게시판', '서울캠 자유게시판'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b137ce86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50358785",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffeeeebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.9.0-py3-none-any.whl (6.5 MB)\n",
      "Collecting certifi>=2021.10.8\n",
      "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\apf_temp_admin\\anaconda3\\lib\\site-packages (from selenium) (1.26.4)\n",
      "Collecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.10.2-py3-none-any.whl (17 kB)\n",
      "Collecting trio~=0.17\n",
      "  Downloading trio-0.22.0-py3-none-any.whl (384 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\apf_temp_admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.10)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\apf_temp_admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.5)\n",
      "Collecting exceptiongroup>=1.0.0rc9\n",
      "  Downloading exceptiongroup-1.1.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\apf_temp_admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\apf_temp_admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\apf_temp_admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.3.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\apf_temp_admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (20.3.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\apf_temp_admin\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\apf_temp_admin\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: outcome, h11, exceptiongroup, wsproto, trio, trio-websocket, certifi, selenium\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2020.12.5\n",
      "    Uninstalling certifi-2020.12.5:\n",
      "      Successfully uninstalled certifi-2020.12.5\n",
      "Successfully installed certifi-2022.12.7 exceptiongroup-1.1.1 h11-0.14.0 outcome-1.2.0 selenium-4.9.0 trio-0.22.0 trio-websocket-0.10.2 wsproto-1.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.4 requires pathlib, which is not installed.\n",
      "anaconda-project 0.9.1 requires ruamel-yaml, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8605abc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a946bb44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19de37c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
